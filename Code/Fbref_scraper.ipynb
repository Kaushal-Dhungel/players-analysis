{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae86f5e9",
   "metadata": {},
   "source": [
    "### Code used to scrape FBREF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca80a9",
   "metadata": {},
   "source": [
    "### 1. SCRAPE ALL LEAGUE TEAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import html5lib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all league teams\n",
    "fbref_league_names = {\n",
    "    \"pl\":[9, \"Premier-League\", 20],\n",
    "    \"seriea\":[11, \"Serie-A\", 20],\n",
    "    \"laliga\":[12, \"La-Liga\", 20],\n",
    "    \"bundesliga\":[20, \"Bundesliga\", 18],\n",
    "    \"league1\":[13, \"Ligue-1\", 18],\n",
    "    \"eredivisie\": [23, \"Eredivisie\", 19]\n",
    "}\n",
    "\n",
    "fbref_scrapped = {}\n",
    "def scrape_league_teams(code, name, number):\n",
    "\n",
    "    url = f'https://fbref.com/en/comps/{code}/{name}-Stats'\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                    \"Chrome/122.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "                \"image/avif,image/webp,image/apng,*/*;q=0.8,\"\n",
    "                \"application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\", \n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    anchors = soup.find_all('a', href=re.compile(r'^/en/squads/'))\n",
    "    anchors = anchors[:number+1]\n",
    "    fbref_scrapped[name] = anchors\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRAPING ALL THE LEAGUE TEAMS\n",
    "for key,val in fbref_league_names.items():\n",
    "    scrape_league_teams(val[0], val[1], val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVED ALL TEAMS URL TO A FILE\n",
    "def extract_hrefs(data):\n",
    "    result = {}\n",
    "    for league, tags in data.items():\n",
    "        hrefs = []\n",
    "        for tag in tags:\n",
    "            if hasattr(tag, 'get'):\n",
    "                href = tag.get('href')\n",
    "                if href:\n",
    "                    hrefs.append(href)\n",
    "        result[league] = hrefs\n",
    "    return result\n",
    "\n",
    "cleaned_data = extract_hrefs(fbref_scrapped)\n",
    "\n",
    "# Write to file\n",
    "with open('fbref_team_links.json', 'w') as f:\n",
    "    json.dump(cleaned_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e860f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2dbd6cf",
   "metadata": {},
   "source": [
    "### 2. SCRAPE ALL PLAYERS LINK FROM EACH TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_all_attackers = {}\n",
    "\n",
    "def scrape_team_attackers(league_name,league_urls):\n",
    "    all_attackers = []\n",
    "    for team_url in league_urls:\n",
    "        if team_url == \"/en/squads/\":\n",
    "            continue\n",
    "        \n",
    "        url = f\"https://fbref.com/{team_url}\"\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                        \"Chrome/122.0.0.0 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "                    \"image/avif,image/webp,image/apng,*/*;q=0.8,\"\n",
    "                    \"application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "            \"DNT\": \"1\",  # Do Not Track\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        anchors = soup.find_all('a', href=re.compile(r'^/en/players/'))\n",
    "        html = pd.read_html(url)\n",
    "        df = html[0]\n",
    "        df.columns = df.columns.get_level_values(-1)\n",
    "        df = df[df[\"Pos\"].str.contains(\"FW\", na=False)]\n",
    "        attackers_list = df[\"Player\"].to_list()\n",
    "        filtered_tags = [\n",
    "            tag for tag in anchors\n",
    "            if tag.text in attackers_list and \"matchlogs\" not in tag['href']\n",
    "        ]\n",
    "\n",
    "        all_attackers += filtered_tags\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    league_all_attackers[league_name] = all_attackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fbref_team_links.json', 'r') as f:\n",
    "    clubs_link_data = json.load(f)\n",
    "\n",
    "for key,val in clubs_link_data:\n",
    "    scrape_team_attackers(key, clubs_link_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = extract_hrefs(league_all_attackers)\n",
    "\n",
    "with open('attackers/all_attackers.json', 'w') as f:\n",
    "    json.dump(cleaned_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd54b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b4ea31",
   "metadata": {},
   "source": [
    "### 3. SCRAPE PLAYERS STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_urls_to_scouting(urls):\n",
    "    base = \"https://fbref.com\"\n",
    "    return [\n",
    "        f\"{base}{match.group(1)}/scout/365_m1/{match.group(2)}-Scouting-Report\"\n",
    "        for url in urls\n",
    "        if (match := re.match(r\"(/en/players/[\\w\\d]+)/([\\w\\-]+)\", url))\n",
    "    ]\n",
    "\n",
    "folders = os.listdir(\"players\")\n",
    "players_name_list = []\n",
    "\n",
    "for f in folders:\n",
    "    players_name_list += [\n",
    "            filename.replace(\".csv\", \"\")\n",
    "            for filename in os.listdir(f\"players/{f}\")\n",
    "            if filename.endswith(\".csv\")\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_players_data(league,urls):\n",
    "    urls = list(set(urls))  # remove duplicates\n",
    "    urls_formatted = convert_urls_to_scouting(urls)\n",
    "    \n",
    "    for url in urls_formatted:\n",
    "        last_part = url.rstrip('/').split(\"/\")[-1]\n",
    "        player_name = last_part.replace(\"-Scouting-Report\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "        # if player_name not in rescrape:\n",
    "        #     continue\n",
    "        # if player_name in players_name_list:\n",
    "        #     continue\n",
    "\n",
    "        try:\n",
    "            tables = pd.read_html(url)\n",
    "            if len(tables) > 3:\n",
    "                table = tables[-2]\n",
    "            \n",
    "            else:\n",
    "                table = tables[2]\n",
    "            table.to_csv(f\"players/{league}/{player_name}.csv\")\n",
    "        except Exception as e:\n",
    "            print(F\"ERROR FETCHING {player_name} data\", e)\n",
    "\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb09358",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('attackers/all_attackers.json', 'r') as f:\n",
    "    players_link_data = json.load(f)\n",
    "\n",
    "for key,val in players_link_data.items():\n",
    "    scrape_players_data(key, players_link_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14140cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2db77d6",
   "metadata": {},
   "source": [
    "### 4. CLEAN AND MERGE ALL PLAYERS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THESE ARE THE METRICS I AM MOST INTERESTED IN\n",
    "search_terms = [\n",
    "    \"Goals\",\n",
    "    \"Assists\",\n",
    "    \"Non-Penalty Goals\",\n",
    "    \"xG: Expected Goals\",\n",
    "    \"npxG: Non-Penalty xG\", \n",
    "    \"xAG: Exp. Assisted Goals\",\n",
    "    \"Progressive Carries\",\n",
    "    \"Progressive Passes\",\n",
    "    \"Progressive Passes Rec\",\n",
    "    \"Shots Total\",\n",
    "    \"Shots on Target\",\n",
    "    \"Goals/Shot\",\n",
    "    \"npxG/Shot\",\n",
    "    \"xA: Expected Assists\",\n",
    "    \"Key Passes\",\n",
    "    \"Through Balls\",\n",
    "    \"Crosses\",\n",
    "    \"Shot-Creating Actions\",\n",
    "    \"Goal-Creating Actions\",\n",
    "    \"Shots on Target\",\n",
    "    \"Touches (Att 3rd)\",\n",
    "    \"Touches (Att Pen)\",\n",
    "]\n",
    "\n",
    "whole_df = pd.DataFrame(columns=[\"Name\", \"League\"] + search_terms)\n",
    "whole_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(\"players\")\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"players/{folder}\")\n",
    "    for file in files:\n",
    "        df = pd.read_csv(f\"players/{folder}/{file}\")\n",
    "        player_name = file.replace(\".csv\", \"\")\n",
    "        \n",
    "        player_data = {key: None for key in [\"Name\"] + search_terms}\n",
    "        player_data[\"Name\"] = player_name\n",
    "        player_data[\"League\"] = folder\n",
    "\n",
    "        for term in search_terms:\n",
    "            # mask = df.apply(lambda row: row.astype(str).str.contains(term, na=False)).any(axis=1)\n",
    "            mask = df.apply(lambda row: row.astype(str).str.contains(term, case=False, na=False, regex=False)).any(axis=1)\n",
    "            if any(mask):\n",
    "                try:\n",
    "                    first_row = df[mask].iloc[0]\n",
    "                    row_val = first_row[\"Standard Stats.1\"]\n",
    "                    player_data[term] = row_val\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting '{term}' for {player_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"{term} not found for {player_name}\")\n",
    "\n",
    "        new_row = pd.DataFrame([player_data], columns=whole_df.columns)\n",
    "        whole_df = pd.concat([whole_df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df.to_csv(\"fbref_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
